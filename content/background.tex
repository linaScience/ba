\chapter{Background}\label{ch:background}

\section{Machine Learning}

\subsection{Classification}

Classification models in \ac{ml} are designed to categorize input data into specific classes using input features and labels.
In supervised learning, models are trained with input vectors, and their associated target vectors, which represent the desired output or correct category for each input \cite[p.3]{BishopPatternRecognition}.
Classification problems, like digit recognition, assign input vectors to discrete categories.
Practical applications include determining if an email is spam or not, or assessing sentiment as positive or negative.
Multi-class classification, a subset of classification, differentiates among more than two classes \cite[pp.179-182]{BishopPatternRecognition}.
In this thesis, a multi-class classification model will be used to predict which \ac{ap} a user will be closest to, based on the \ac{rssi} from the \acp{ap} and the user's trajectory.


\subsection{Univariate and Multivariate Time Series}

A time series is univariate, if one observation recorded sequentially over time, e.g., temperature or stock prices.
The focus of using \ac{ml} is to understand and forecast a single variable's behavior.
In the dataset observed in this thesis, has more than just one value which is recorded sequentially over time such as \ac{rssi}, waypoint and sensor data.  
Therefore, the dataset has multiple observations were recorded over the same time intervals, \ac{ml} allows for the analysis of interrelationships and interdependencies between these variables, e.g., temperature, humidity, uv-index and wind speed.
The focus here is on delving into understanding dynamic interactions and co-movements between multiple variables


\subsection{Time Series Prediction}

Time series prediction is a type of supervised learning problem where a model is trained on a sequence of observations and learns to predict the next value in the sequence.
Those sequences consist of data points arranged chronologically, prevalent in numerous domains like stock prices.
Due to its inherent temporal dependencies, where subsequent data points influence previous ones, specific machine learning techniques are applied.
These include \ac{mlp} \cite{mlp-backpropagation-rumelhart}, \ac{hmm} \cite{hmm-rabiner-1989}, and \ac{rnn} \cite{hopfield-rnn} models such as \ac{lstm} \cite{lstm-hochreiter}.
Each model is designed to capture and leverage temporal patterns within the data, predicting future trends based on historical observations \cite{neptune-ai}.


% Machine learning models
\subsection{Multilayer Perceptron}

\ac{mlp}, also known as a feedforward artificial neural network, is a class of deep learning models primarily used for supervised learning tasks.
An MLP consists of multiple layers of nodes in a directed graph, each fully connected to the next one.
Each node in one layer is connected with certain weights to every node in the following layer.
\acp{mlp} apply a series of transformations, typically nonlinear, to the input data using activation functions, such as the sigmoid or \ac{relu}, facilitating the model's ability to model complex patterns and dependencies in the data \cite{goodfellow_deep_2016}.

\subsection{Hidden Markov Model}

\ac{hmm} is a statistical model that assumes the system being modeled is a Markov process with unobserved states.
\acp{hmm} are mainly known for their application in temporal pattern recognition, such as speech and handwriting.
They describe the probability of a sequence of observable data, which is assumed to result from a sequence of hidden states, each producing an observable output according to a particular probability distribution.

\subsection{Recurrent Neural Networks}

\ac{rnn} is an artificial neural network well-suited to sequential data because of its intrinsic design.
Unlike traditional feedforward neural networks, an \ac{rnn} possesses loops in its topology, allowing information to persist over time.
This unique characteristic enables the model to use its internal state (memory) to process sequences of inputs, making it ideally suited for tasks involving sequential data such as speech recognition, language modeling, and time series prediction \cite{elman_finding_1990}.

\subsubsection{Long Short-Term Memory}

\ac{lstm} is a special kind of RNN, capable of learning long-term dependencies.
\acp{lstm} were designed to combat the ``vanishing gradient'' problem in traditional \acp{rnn}. 
This problem made it difficult for other neural networks to learn from data where relevant events occurred with significant gaps between them.
The key to the ability of the \acp{lstm} is its cell state and the accompanying gates (input, forget, and output gate), which regulate the flow of information in the network.
\acp{lstm} use \texttt{tanh} as activation function.

\subsection{Hyperparameter tuning}

In machine learning, hyperparameters play a vital role in model development.
These are parameters such as the learning rate, neural network layers, and the number of windows or batch sizes.
Proper selection of hyperparameters, known as hyperparameter tuning or optimization, is crucial to optimize model performance.
This iterative procedure involves exploring various hyperparameter combinations for the configuration that yields the most accurate predictions.
Hyperparameters can be tuned by, e.g., random search, which can be done manually or using libraries.
This thesis will use keras-tuner \cite{keras_tuner} to tune the hyperparameters of the \ac{lstm} model.
