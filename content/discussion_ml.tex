\chapter{Suitable Machine Learning Model}\label{sec:discuss-ml}

% \begin{itemize}
%     \item What do we want to predict?
%     \item We want to use supervised learning
%     \item We want to predict the three best BSSIDs for a given location of a user
%     \item We could use regression or classification
%     \item Pro regression: We could predict the exact signal strength
%     \item Con: BSSIDs could not be compared and, though predicted
%     \item Pro classification: BSSIDs could be compared and, though predicted
% \end{itemize}

\begin{itemize}
    \item As seen in the previous chapter, many data for one floor 
    \item As this is a time series, we want to use a time series machine learning model
    \item As we want to predict the next BSSID out of many BSSIDs, so classification problem with supervised learning
    \item Discussion of some pre-chosen models and decision for one which will be implemented in the next chapter
\end{itemize}

\section{Classification Models}
\begin{itemize}
    \item Stated in \cref{sec:background}, classification model in ML is a type of predictive model that categorizes incoming input data into specific classes
    \item Prediction of next BSSID is a classification problem
    \item therefore, interpret the problem as a classification problem
    \item For multivariate time series classification models, there are only a few models
    \begin{itemize}
        \item \acp{mlp} \cite{TSC}
        \item \acp{rnn} such as \acp{lstm}\cite{lstm-hochreiter}
        \item \acp{hmm} \cite{hmm-movement-prediction}, 
    \end{itemize}
\end{itemize}


% Machine learning models
\subsection{\ac{mlp}}

\ac{mlp}, also known as a feedforward artificial neural network, is a class of deep learning models primarily used for supervised learning tasks.
An MLP consists of multiple layers of nodes in a directed graph, each fully connected to the next one.
Each node in one layer is connected with certain weights to every node in the following layer.
\acp{mlp} apply a series of transformations, typically nonlinear, to the input data using activation functions, such as the sigmoid or \ac{relu}, facilitating the model's ability to model complex patterns and dependencies in the data \cite{goodfellow_deep_2016}.

\subsection{\ac{hmm}}

\ac{hmm} is a statistical model that assumes the system being modeled is a Markov process with unobserved (hidden) states\cite{hmm-rabiner-1989}.
\acp{hmm} are mainly known for their application in temporal pattern recognition, such as speech and handwriting.
They describe the probability of a sequence of observable data, which is assumed to result from a sequence of hidden states, each producing an observable output according to a particular probability distribution.

\subsection{\ac{rnn}}

\ac{rnn} is an artificial neural network well-suited to sequential data because of its intrinsic design.
Unlike traditional feedforward neural networks, an \ac{rnn} possesses loops in its topology, allowing information to persist over time.
This unique characteristic enables the model to use its internal state (memory) to process sequences of inputs, making it ideally suited for tasks involving sequential data such as speech recognition, language modeling, and time series prediction\cite{elman_finding_1990}.

\subsubsection{\ac{lstm}}

\ac{lstm} is a special kind of RNN, capable of learning long-term dependencies, which Hochreiter and Schmidhuber introduced in 1997\cite{lstm-hochreiter}.
\acp{lstm} were designed to combat the ``vanishing gradient'' problem in traditional \acp{rnn}. 
This problem made it difficult for other neural networks to learn from data where relevant events occurred with significant gaps between them.
The key to the ability of the \acp{lstm} is its cell state and the accompanying gates (input, forget, and output gate), which regulate the flow of information in the network.

% \subsubsection{\ac{gru}}

% \ac{gru} is a type of \ac{rnn} introduced by Cho et al. in 2014 as a simpler variant of the LSTM.
% \acp{gru} use gating mechanisms similar to those in the LSTM but combine the cell state and hidden state while also using fewer gates, which can make them more computationally efficient\cite{cho_learning_2014}.
% This simplified structure has been shown to perform comparably to the LSTM on specific tasks.

\subsection{Discussion of Classification Models}
As mentioned in \cref{sec:data-ana}, the floor analyzed there has 4795 \acp{bssid}.
So we have 4795 classes for the classification problem.
The selection of a suitable model for this task is even more critical.
We will discuss the classification models by the following topics: Temporal Dependency Handling, Capacity and Complexity, Multivariate Data, Flexibility and Integration, and Regularization and Overfitting. \\

\textbf{Temporal Dependency Handling:}
\begin{itemize}
    \item \acp{lstm}, by design, are equipped to handle long-term temporal dependencies. Their unique cell state and gating mechanisms allow them to store, modify, and access information over extended periods, making them adept at capturing patterns from long sequences.
    \item \acp{mlp} lack a built-in mechanism for remembering past information, making them less suitable for time series data where temporal order and dependencies are crucial.
    \item While \acp{hmm} can handle temporal dependencies to some extent, they often struggle with longer sequences and multivariate data due to their Markovian assumption, which limits their memory to the most recent state.
    \item Standard RNNs were designed to handle temporal dependencies, but they suffer the vanishing gradient problem, making them less effective in capturing long-term dependencies compared to \acp{lstm}\cite{rnn_difficulties_2013}.
\end{itemize}

\textbf{Capacity and Complexity:}
With 4,795 classes, the model needs a considerable capacity to differentiate between the subtle differences in patterns that might exist among them. 
\acp{lstm}, being deep learning models, can scale effectively in terms of capacity by adding more layers or units while still maintaining their ability to handle temporal data.\\

\textbf{Multivariate Data:}
\begin{itemize}
    \item \acp{lstm} can seamlessly handle multivariate time series data. Their recurrent nature allows them to effectively process each time step with multiple features.
    \item While \acp{mlp} can also handle multivariate data, they treat each feature and time step independently, often missing out on the interdependencies.
    \item \acp{hmm} are primarily designed for univariate data. Extending them to multivariate scenarios requires additional complexities and assumptions.
\end{itemize}

\textbf{Flexibility and Integration:}
\acp{lstm} can be easily integrated with other deep learning architectures, such as Convolutional Neural Networks (CNNs), to capture both temporal and spatial features. 
This flexibility is advantageous when dealing with complex and varied data sources.\\

\textbf{Regularization and Overfitting:}
Deep learning models, including \acp{lstm}, come with many regularization techniques, such as dropout, which can be crucial when dealing with many classes and the risk of overfitting.\\

In conclusion, while \acp{mlp}, \acp{hmm}, and traditional RNNs have their strengths and have been successful in many applications, they have problems with multivariate time series classification with many classes.
This problem demands a model that can efficiently capture intricate temporal patterns, scale in capacity, and handle multivariate data.
\acp{lstm}, with their unique architecture and properties, can deal with this challenge, making them the preferred choice for this task and the selected model for our implementation.

%\noindent
